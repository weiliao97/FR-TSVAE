{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and compare trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint file \n",
    "import glob \n",
    "base_dir = '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/'\n",
    "workname = '0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/'\n",
    "all_path = glob.glob(base_dir+workname + '*.pt')\n",
    "all_path = [p for p in all_path if \"stage2\" not in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_2_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_epoch74.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_0_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_2_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_3_epoch74.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_4_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_0_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_2_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_4_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_0_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_4_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_3_epoch74.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_3_epoch74.pt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nobackup/users/weiliao/mimic_sepsis3.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f08314985612>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_sepsis3\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mtrain_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sepsis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mimic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mdev_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_sofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sepsis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mimic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_sofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mtest_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_sepsis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mimic'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_static\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/content/FR-TSVAE/utils.py\u001b[0m in \u001b[0;36mfilter_sepsis\u001b[0;34m(database, vital, static, sofa, ids)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfilter_sepsis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvital\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msofa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdatabase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mimic'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mid_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/nobackup/users/weiliao/mimic_sepsis3.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0msepsis3_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stay_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m  \u001b[0;31m# 1d array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nobackup/users/weiliao/mimic_sepsis3.csv'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os \n",
    "import copy \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import utils\n",
    "from utils import AverageMeterSet\n",
    "import prepare_data\n",
    "import models\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "date = today.strftime(\"%m%d\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "legend_properties = {'weight':'bold', 'size': 14}\n",
    "dir_data = {'satori': '/nobackup/users/weiliao', 'colab':'/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_sep_2022/0910'}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Parser for time series VAE models\")\n",
    "    parser.add_argument(\"--device_id\", type=int, default=0, help=\"GPU id\")\n",
    "    parser.add_argument(\"--platform\", type=str, default='satori', choices=['satori', 'colab'], help='Platform to run the code')\n",
    "    # data/loss parameters\n",
    "    parser.add_argument(\"--use_sepsis3\", action = 'store_false', default= True, help=\"Whethe only use sepsis3 subset\")\n",
    "    parser.add_argument(\"--bucket_size\", type=int, default=300, help=\"bucket size to group different length of time-series data\")\n",
    "    parser.add_argument(\"--beta\", type=float, default=0.001, help=\"coefficent for the elbo loss\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.5, help=\"coefficent for the total_corr loss\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.5, help=\"coefficent for the clf loss\")\n",
    "    parser.add_argument(\"--theta\", type=float, default=5, help=\"coefficent for the sofa loss in stage 1\")\n",
    "    parser.add_argument(\"--zdim\", type=int, default=20, help=\"dimension of the latent space\")\n",
    "    parser.add_argument(\"--sens_ind\", type=int, default=21, help=\"index of the sensitive feature\")\n",
    "    parser.add_argument(\"--scale_elbo\", action = 'store_true', default=False, help=\"Whether to scale the ELBO loss\")\n",
    "    # model parameters\n",
    "    parser.add_argument(\"--kernel_size\", type=int, default=3, help=\"kernel size\")\n",
    "    parser.add_argument(\"--drop_out\", type=float, default=0.2, help=\"drop out rate\")\n",
    "    parser.add_argument(\"--enc_channels\", default=[256, 64, 16, 2],  help=\"number of channels in the encoder\")\n",
    "    parser.add_argument(\"--dec_channels\", default = [16, 64, 256, 200], help=\"number of channels in the decoder\")\n",
    "    parser.add_argument(\"--num_inputs\", type=int, default=200, help=\"number of features in the inputs\")\n",
    "    # discriminator parameters\n",
    "    parser.add_argument(\"--disc_channels\",  type=int, default=200, help=\"number of channels in the discriminator\")\n",
    "    # regressor parameters\n",
    "    parser.add_argument(\"--regr_model\",  type=str, default='mlp', choices=['mlp', 'tcn'], help='Model choice in sofa prediction')\n",
    "    parser.add_argument(\"--regr_channels\",  type=int, default=200, help=\"number of channels in the regressor\")\n",
    "    parser.add_argument(\"--regr_tcn_channels\",  nargs='+', type=int, help=\"number of channels in the regressor\")\n",
    "    parser.add_argument(\"--regr_only_nonsens\", action = 'store_false', default=True, help=\"Whether only using nonsens latents to predict sofa\")\n",
    "    # training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=300, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--data_batching\", type=str, default='close', choices=['same', 'close', 'random'], help='How to batch data')\n",
    "    parser.add_argument(\"--bs\", type=int, default=16, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=20, help=\"Patience epochs for early stopping.\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, default='test', help=\" name of checkpoint model\")\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "    device = torch.device(\"cuda:%d\"%args.device_id if torch.cuda.is_available() else \"cpu\")\n",
    "    arg_dict = vars(args)\n",
    "    workname = date + \"_\" +  args.checkpoint\n",
    "    # utils.creat_checkpoint_folder('/home/weiliao/FR-TSVAE/checkpoints/' + workname, 'params.json', arg_dict)\n",
    "\n",
    "    # load data\n",
    "    meep_mimic = np.load(dir_data[args.platform] + '/MIMIC_compile_0911_2022.npy', \\\n",
    "                    allow_pickle=True).item()\n",
    "    train_vital = meep_mimic ['train_head']\n",
    "    dev_vital = meep_mimic ['dev_head']\n",
    "    test_vital = meep_mimic ['test_head']\n",
    "    mimic_static = np.load(dir_data[args.platform] + '/MIMIC_static_0922_2022.npy', \\\n",
    "                            allow_pickle=True).item()\n",
    "    mimic_target = np.load(dir_data[args.platform] + '/MIMIC_target_0922_2022.npy', \\\n",
    "                            allow_pickle=True).item()c\n",
    "        \n",
    "    train_head, train_static, train_sofa, train_id =  utils.crop_data_target('mimic', train_vital, mimic_target, mimic_static, 'train', args.sens_ind)\n",
    "    dev_head, dev_static, dev_sofa, dev_id =  utils.crop_data_target('mimic', dev_vital , mimic_target, mimic_static, 'dev',  args.sens_ind)\n",
    "    test_head, test_static, test_sofa, test_id =  utils.crop_data_target('mimic', test_vital, mimic_target, mimic_static, 'test',  args.sens_ind)\n",
    "\n",
    "    if args.use_sepsis3 == True:\n",
    "        train_head, train_static, train_sofa, train_id = utils.filter_sepsis('mimic', train_head, train_static, train_sofa, train_id, args.platform)\n",
    "        dev_head, dev_static, dev_sofa, dev_id = utils.filter_sepsis('mimic', dev_head, dev_static, dev_sofa, dev_id, args.platform)\n",
    "        test_head, test_static, test_sofa, test_id = utils.filter_sepsis('mimic', test_head, test_static, test_sofa, test_id, args.platform)\n",
    "\n",
    "    # build model\n",
    "    model = models.Ffvae(args)\n",
    "    # torch.save(model.state_dict(), '/home/weiliao/FR-TSVAE/start_weights.pt')\n",
    "\n",
    "    # 10-fold cross validation\n",
    "    trainval_head = train_head + dev_head\n",
    "    trainval_static = train_static + dev_static\n",
    "    trainval_stail = train_sofa + dev_sofa\n",
    "    trainval_ids = train_id + dev_id\n",
    "\n",
    "    # prepare data\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for c_fold, (train_index, test_index) in enumerate(kf.split(trainval_head)):\n",
    "        # best_loss = 1e4\n",
    "        # patience = 0\n",
    "        # if c_fold >= 1:\n",
    "        #     model.load_state_dict(torch.load('/home/weiliao/FR-TSVAE/start_weights.pt'))\n",
    "        print('Starting Fold %d' % c_fold)\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        train_head, val_head = utils.slice_data(trainval_head, train_index), utils.slice_data(trainval_head, test_index)\n",
    "        train_static, val_static = utils.slice_data(trainval_static, train_index), utils.slice_data(trainval_static, test_index)\n",
    "        train_stail, val_stail = utils.slice_data(trainval_stail, train_index), utils.slice_data(trainval_stail, test_index)\n",
    "        train_id, val_id = utils.slice_data(trainval_ids, train_index), utils.slice_data(trainval_ids, test_index)\n",
    "\n",
    "        train_dataloader, dev_dataloader, test_dataloader = prepare_data.get_data_loader(args, train_head, val_head,\n",
    "                                                                                            test_head, \n",
    "                                                                                            train_stail, val_stail,\n",
    "                                                                                            test_sofa,\n",
    "                                                                                            train_static=train_static,\n",
    "                                                                                            dev_static=val_static,\n",
    "                                                                                            test_static=test_static,\n",
    "                                                                                            train_id=train_id,\n",
    "                                                                                            dev_id=val_id,\n",
    "                                                                                            test_id=test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns \n",
    "import torch.nn as nn \n",
    "\n",
    "for p in all_path: \n",
    "    model.load_state_dict(torch.load(p))\n",
    "    #  test on test loader \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    stt = []\n",
    "    with torch.no_grad():\n",
    "        for vitals, static, target, train_ids, key_mask in test_dataloader:\n",
    "            # (bs, feature_dim, T)\n",
    "            vitals = vitals.to(device)\n",
    "            # (bs)\n",
    "            static = static.to(device)\n",
    "            # (bs, T, 1)\n",
    "            target = target.to(device)\n",
    "            # (bs, T)\n",
    "            key_mask = key_mask.to(device)\n",
    "\n",
    "            # _mu shape [bs, zdim]\n",
    "            _mu, _logvar = model.encoder(vitals)\n",
    "            # b_logits [bs, 1]\n",
    "            b_logits = _mu[:, model.sens_idx]\n",
    "            logits.extend(b_logits)\n",
    "            stt.extend(static)\n",
    "    logits = torch.stack(logits)\n",
    "    stt = torch.stack(stt) \n",
    "    metrics.RocCurveDisplay.from_predictions(stt.cpu(),  nn.Sigmoid()(logits).cpu())  \n",
    "    plt.show()\n",
    "    wname = p.split('/')[-1].split('.')[0]\n",
    "    # plt.savefig('./checkpoints/' + workname + 'auc_curve/' + '%s_auc.eps'%wname, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "    pred =  (nn.Sigmoid()(logits).cpu() > 0.5).float()\n",
    "    cm = metrics.confusion_matrix(stt.cpu(), pred)\n",
    "    cf_matrix = cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
    "    # percentage based on true label \n",
    "    gr = (cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)).flatten()\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in gr]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, group_counts)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "\n",
    "    xlabel = ['Pred-%d'%i for i in range(2)]\n",
    "    ylabel = ['%d'%i for i in range(2)]\n",
    "\n",
    "    sns.set(font_scale = 1.5)\n",
    "\n",
    "    hm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap = 'OrRd', \\\n",
    "    annot_kws={\"fontsize\": 16}, xticklabels=xlabel, yticklabels=ylabel, cbar=False)\n",
    "    # hm.set(title=title)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()  \n",
    "    # plt.savefig('./checkpoints/' + workname + 'cm_matrix/' + '%s_cm.eps'%wname, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
