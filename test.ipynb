{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and compare trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint file \n",
    "import glob \n",
    "base_dir = '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/'\n",
    "workname = '0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/'\n",
    "all_path = glob.glob(base_dir+workname + '*.pt')\n",
    "all_path = [p for p in all_path if \"stage2\" not in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(base_dir+workname+'params.json') as f: \n",
    "    params = json.load(f)\n",
    "params['platform'] = 'colab'\n",
    "class Args:\n",
    "    def __init__(self, d=None):\n",
    "        if d is not None:\n",
    "            for key, value in d.items():\n",
    "                setattr(self, key, value)\n",
    "args = Args(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_2_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_epoch74.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_0_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_2_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_3_epoch74.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_4_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_0_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_2_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_4_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_sofa_fold_1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_0_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_1_epoch40.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_4_epoch149.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_epoch101.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_fold_3_epoch74.pt',\n",
       " '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/stage1_clf_fold_3_epoch74.pt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 0\n",
      "TRAIN: 8964 TEST: 2242\n",
      "8964\n",
      "Starting Fold 1\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "Starting Fold 2\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "Starting Fold 3\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "Starting Fold 4\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os \n",
    "import copy \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import utils\n",
    "from utils import AverageMeterSet\n",
    "import prepare_data\n",
    "import models\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "date = today.strftime(\"%m%d\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "legend_properties = {'weight':'bold', 'size': 14}\n",
    "dir_data = {'satori': '/nobackup/users/weiliao', 'colab':'/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_sep_2022/0910'}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Parser for time series VAE models\")\n",
    "    parser.add_argument(\"--device_id\", type=int, default=0, help=\"GPU id\")\n",
    "    parser.add_argument(\"--platform\", type=str, default='colab', choices=['satori', 'colab'], help='Platform to run the code')\n",
    "    # data/loss parameters\n",
    "    parser.add_argument(\"--use_sepsis3\", action = 'store_false', default= True, help=\"Whethe only use sepsis3 subset\")\n",
    "    parser.add_argument(\"--bucket_size\", type=int, default=300, help=\"bucket size to group different length of time-series data\")\n",
    "    parser.add_argument(\"--beta\", type=float, default=0.001, help=\"coefficent for the elbo loss\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.5, help=\"coefficent for the total_corr loss\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.5, help=\"coefficent for the clf loss\")\n",
    "    parser.add_argument(\"--theta\", type=float, default=5, help=\"coefficent for the sofa loss in stage 1\")\n",
    "    parser.add_argument(\"--zdim\", type=int, default=20, help=\"dimension of the latent space\")\n",
    "    parser.add_argument(\"--sens_ind\", type=int, default=21, help=\"index of the sensitive feature\")\n",
    "    parser.add_argument(\"--scale_elbo\", action = 'store_true', default=False, help=\"Whether to scale the ELBO loss\")\n",
    "    # model parameters\n",
    "    parser.add_argument(\"--kernel_size\", type=int, default=3, help=\"kernel size\")\n",
    "    parser.add_argument(\"--drop_out\", type=float, default=0.2, help=\"drop out rate\")\n",
    "    parser.add_argument(\"--enc_channels\", default=[256, 128, 64, 40],  help=\"number of channels in the encoder\")\n",
    "    parser.add_argument(\"--dec_channels\", default = [64, 128, 256, 200], help=\"number of channels in the decoder\")\n",
    "    parser.add_argument(\"--num_inputs\", type=int, default=200, help=\"number of features in the inputs\")\n",
    "    # discriminator parameters\n",
    "    parser.add_argument(\"--disc_channels\",  type=int, default=200, help=\"number of channels in the discriminator\")\n",
    "    # regressor parameters\n",
    "    parser.add_argument(\"--regr_model\",  type=str, default='mlp', choices=['mlp', 'tcn'], help='Model choice in sofa prediction')\n",
    "    parser.add_argument(\"--regr_channels\",  type=int, default=200, help=\"number of channels in the regressor\")\n",
    "    parser.add_argument(\"--regr_tcn_channels\",  nargs='+', type=int, help=\"number of channels in the regressor\")\n",
    "    parser.add_argument(\"--regr_only_nonsens\", action = 'store_false', default=True, help=\"Whether only using nonsens latents to predict sofa\")\n",
    "    # training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=300, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--data_batching\", type=str, default='close', choices=['same', 'close', 'random'], help='How to batch data')\n",
    "    parser.add_argument(\"--bs\", type=int, default=16, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=20, help=\"Patience epochs for early stopping.\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, default='test', help=\" name of checkpoint model\")\n",
    "\n",
    "    # args = parser.parse_known_args()[0]\n",
    "    device = torch.device(\"cuda:%d\"%args.device_id if torch.cuda.is_available() else \"cpu\")\n",
    "    arg_dict = vars(args)\n",
    "    # workname = date + \"_\" +  args.checkpoint\n",
    "    # utils.creat_checkpoint_folder('/home/weiliao/FR-TSVAE/checkpoints/' + workname, 'params.json', arg_dict)\n",
    "\n",
    "    # load data\n",
    "    meep_mimic = np.load(dir_data[args.platform] + '/MIMIC_compile_0911_2022.npy', \\\n",
    "                    allow_pickle=True).item()\n",
    "    train_vital = meep_mimic ['train_head']\n",
    "    dev_vital = meep_mimic ['dev_head']\n",
    "    test_vital = meep_mimic ['test_head']\n",
    "    mimic_static = np.load(dir_data[args.platform] + '/MIMIC_static_0922_2022.npy', \\\n",
    "                            allow_pickle=True).item()\n",
    "    mimic_target = np.load(dir_data[args.platform] + '/MIMIC_target_0922_2022.npy', \\\n",
    "                            allow_pickle=True).item()\n",
    "        \n",
    "    train_head, train_static, train_sofa, train_id =  utils.crop_data_target('mimic', train_vital, mimic_target, mimic_static, 'train', args.sens_ind)\n",
    "    dev_head, dev_static, dev_sofa, dev_id =  utils.crop_data_target('mimic', dev_vital , mimic_target, mimic_static, 'dev',  args.sens_ind)\n",
    "    test_head, test_static, test_sofa, test_id =  utils.crop_data_target('mimic', test_vital, mimic_target, mimic_static, 'test',  args.sens_ind)\n",
    "\n",
    "    if args.use_sepsis3 == True:\n",
    "        train_head, train_static, train_sofa, train_id = utils.filter_sepsis('mimic', train_head, train_static, train_sofa, train_id, args.platform)\n",
    "        dev_head, dev_static, dev_sofa, dev_id = utils.filter_sepsis('mimic', dev_head, dev_static, dev_sofa, dev_id, args.platform)\n",
    "        test_head, test_static, test_sofa, test_id = utils.filter_sepsis('mimic', test_head, test_static, test_sofa, test_id, args.platform)\n",
    "\n",
    "    # build model\n",
    "    model = models.Ffvae(args)\n",
    "    # torch.save(model.state_dict(), '/home/weiliao/FR-TSVAE/start_weights.pt')\n",
    "\n",
    "    # 10-fold cross validation\n",
    "    trainval_head = train_head + dev_head\n",
    "    trainval_static = train_static + dev_static\n",
    "    trainval_stail = train_sofa + dev_sofa\n",
    "    trainval_ids = train_id + dev_id\n",
    "\n",
    "    # prepare data\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for c_fold, (train_index, test_index) in enumerate(kf.split(trainval_head)):\n",
    "        # best_loss = 1e4\n",
    "        # patience = 0\n",
    "        # if c_fold >= 1:\n",
    "        #     model.load_state_dict(torch.load('/home/weiliao/FR-TSVAE/start_weights.pt'))\n",
    "        print('Starting Fold %d' % c_fold)\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        train_head, val_head = utils.slice_data(trainval_head, train_index), utils.slice_data(trainval_head, test_index)\n",
    "        train_static, val_static = utils.slice_data(trainval_static, train_index), utils.slice_data(trainval_static, test_index)\n",
    "        train_stail, val_stail = utils.slice_data(trainval_stail, train_index), utils.slice_data(trainval_stail, test_index)\n",
    "        train_id, val_id = utils.slice_data(trainval_ids, train_index), utils.slice_data(trainval_ids, test_index)\n",
    "\n",
    "        train_dataloader, dev_dataloader, test_dataloader = prepare_data.get_data_loader(args, train_head, val_head,\n",
    "                                                                                            test_head, \n",
    "                                                                                            train_stail, val_stail,\n",
    "                                                                                            test_sofa,\n",
    "                                                                                            train_static=train_static,\n",
    "                                                                                            dev_static=val_static,\n",
    "                                                                                            test_static=test_static,\n",
    "                                                                                            train_id=train_id,\n",
    "                                                                                            dev_id=val_id,\n",
    "                                                                                            test_id=test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with CM and AUC plots\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns \n",
    "import torch.nn as nn \n",
    "\n",
    "for p in all_path: \n",
    "    model.load_state_dict(torch.load(p))\n",
    "    print(p)\n",
    "    #  test on test loader \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    stt = []\n",
    "    with torch.no_grad():\n",
    "        for vitals, static, target, train_ids, key_mask in test_dataloader:\n",
    "            # (bs, feature_dim, T)\n",
    "            vitals = vitals.to(device)\n",
    "            # (bs)\n",
    "            static = static.to(device)\n",
    "            # (bs, T, 1)\n",
    "            target = target.to(device)\n",
    "            # (bs, T)\n",
    "            key_mask = key_mask.to(device)\n",
    "\n",
    "            # _mu shape [bs, zdim, T]\n",
    "            _mu, _logvar = model.encoder(vitals)\n",
    "            # b_logits [bs, 1]\n",
    "            b_logits = _mu[:, model.sens_idx]\n",
    "            logits.extend(torch.stack([b_logits[i].squeeze(0).mean() for i in range(len(b_logits))]))\n",
    "            stt.extend(static)\n",
    "    logits = torch.stack(logits)\n",
    "    stt = torch.stack(stt) \n",
    "    metrics.RocCurveDisplay.from_predictions(stt.cpu(),  nn.Sigmoid()(logits).cpu())  \n",
    "    plt.show()\n",
    "    wname = p.split('/')[-1].split('.')[0]\n",
    "    # plt.savefig('./checkpoints/' + workname + 'auc_curve/' + '%s_auc.eps'%wname, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "    pred =  (nn.Sigmoid()(logits).cpu() > 0.5).float()\n",
    "    cm = metrics.confusion_matrix(stt.cpu(), pred)\n",
    "    cf_matrix = cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
    "    # percentage based on true label \n",
    "    gr = (cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)).flatten()\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in gr]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, group_counts)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "\n",
    "    xlabel = ['Pred-%d'%i for i in range(2)]\n",
    "    ylabel = ['%d'%i for i in range(2)]\n",
    "\n",
    "    sns.set(font_scale = 1.5)\n",
    "\n",
    "    hm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap = 'OrRd', \\\n",
    "    annot_kws={\"fontsize\": 16}, xticklabels=xlabel, yticklabels=ylabel, cbar=False)\n",
    "    # hm.set(title=title)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()  \n",
    "    # plt.savefig('./checkpoints/' + workname + 'cm_matrix/' + '%s_cm.eps'%wname, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFA Test set results, including MSE, analyze on 20 paticular patients, MSE variation by ICU stay length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the same with second stage SOFA training "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
