{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and compare trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint file \n",
    "import glob \n",
    "base_dir = '/content/drive/My Drive/ColabNotebooks/MIMIC/TCN/VAE/checkpoints/'\n",
    "workname = '0507_lr1e-4beta.001_res_regrtheta_5_mlp_regr_nonsens_21/'\n",
    "all_path = glob.glob(base_dir+workname + '*.pt')\n",
    "all_path = [p for p in all_path if \"stage2\" not in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(base_dir+workname+'params.json') as f: \n",
    "    params = json.load(f)\n",
    "params['platform'] = 'colab'\n",
    "class Args:\n",
    "    def __init__(self, d=None):\n",
    "        if d is not None:\n",
    "            for key, value in d.items():\n",
    "                setattr(self, key, value)\n",
    "args = Args(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os \n",
    "import copy \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import utils\n",
    "from utils import AverageMeterSet\n",
    "import prepare_data\n",
    "import models\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "date = today.strftime(\"%m%d\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "legend_properties = {'weight':'bold', 'size': 14}\n",
    "dir_data = {'satori': '/nobackup/users/weiliao', 'colab':'/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_sep_2022/0910'}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Parser for time series VAE models\")\n",
    "    parser.add_argument(\"--device_id\", type=int, default=0, help=\"GPU id\")\n",
    "    parser.add_argument(\"--platform\", type=str, default='satori', choices=['satori', 'colab'], help='Platform to run the code')\n",
    "    # data/loss parameters\n",
    "    parser.add_argument(\"--use_sepsis3\", action = 'store_false', default= True, help=\"Whethe only use sepsis3 subset\")\n",
    "    parser.add_argument(\"--bucket_size\", type=int, default=300, help=\"bucket size to group different length of time-series data\")\n",
    "    parser.add_argument(\"--beta\", type=float, default=0.001, help=\"coefficent for the elbo loss\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.5, help=\"coefficent for the total_corr loss\")\n",
    "    parser.add_argument(\"--alpha\", type=float, default=0.5, help=\"coefficent for the clf loss\")\n",
    "    parser.add_argument(\"--theta\", type=float, default=5, help=\"coefficent for the sofa loss in stage 1\")\n",
    "    parser.add_argument(\"--zdim\", type=int, default=20, help=\"dimension of the latent space\")\n",
    "    parser.add_argument(\"--sens_ind\", type=int, default=21, help=\"index of the sensitive feature\")\n",
    "    parser.add_argument(\"--scale_elbo\", action = 'store_true', default=False, help=\"Whether to scale the ELBO loss\")\n",
    "    # model parameters\n",
    "    parser.add_argument(\"--kernel_size\", type=int, default=3, help=\"kernel size\")\n",
    "    parser.add_argument(\"--drop_out\", type=float, default=0.2, help=\"drop out rate\")\n",
    "    parser.add_argument(\"--enc_channels\", default=[256, 128, 64, 40],  help=\"number of channels in the encoder\")\n",
    "    parser.add_argument(\"--dec_channels\", default = [64, 128, 256, 200], help=\"number of channels in the decoder\")\n",
    "    parser.add_argument(\"--num_inputs\", type=int, default=200, help=\"number of features in the inputs\")\n",
    "    # discriminator parameters\n",
    "    parser.add_argument(\"--disc_channels\",  type=int, default=200, help=\"number of channels in the discriminator\")\n",
    "    # regressor parameters\n",
    "    parser.add_argument(\"--regr_model\",  type=str, default='mlp', choices=['mlp', 'tcn'], help='Model choice in sofa prediction')\n",
    "    parser.add_argument(\"--regr_channels\",  type=int, default=200, help=\"number of channels in the regressor\")\n",
    "    parser.add_argument(\"--regr_tcn_channels\",  nargs='+', type=int, help=\"number of channels in the regressor\")\n",
    "    parser.add_argument(\"--regr_only_nonsens\", action = 'store_false', default=True, help=\"Whether only using nonsens latents to predict sofa\")\n",
    "    # training parameters\n",
    "    parser.add_argument(\"--epochs\", type=int, default=300, help=\"Number of training epochs\")\n",
    "    parser.add_argument(\"--data_batching\", type=str, default='close', choices=['same', 'close', 'random'], help='How to batch data')\n",
    "    parser.add_argument(\"--bs\", type=int, default=16, help=\"batch size\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-4, help=\"Learning rate\")\n",
    "    parser.add_argument(\"--patience\", type=int, default=20, help=\"Patience epochs for early stopping.\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, default='test', help=\" name of checkpoint model\")\n",
    "\n",
    "    args = parser.parse_known_args()[0]\n",
    "    device = torch.device(\"cuda:%d\"%args.device_id if torch.cuda.is_available() else \"cpu\")\n",
    "    arg_dict = vars(args)\n",
    "    # workname = date + \"_\" +  args.checkpoint\n",
    "    # utils.creat_checkpoint_folder('/home/weiliao/FR-TSVAE/checkpoints/' + workname, 'params.json', arg_dict)\n",
    "\n",
    "    # load data\n",
    "    meep_mimic = np.load(dir_data[args.platform] + '/MIMIC_compile_0911_2022.npy', \\\n",
    "                    allow_pickle=True).item()\n",
    "    train_vital = meep_mimic ['train_head']\n",
    "    dev_vital = meep_mimic ['dev_head']\n",
    "    test_vital = meep_mimic ['test_head']\n",
    "    mimic_static = np.load(dir_data[args.platform] + '/MIMIC_static_0922_2022.npy', \\\n",
    "                            allow_pickle=True).item()\n",
    "    mimic_target = np.load(dir_data[args.platform] + '/MIMIC_target_0922_2022.npy', \\\n",
    "                            allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_key = 'static_' + 'test'\n",
    "idx = pd.IndexSlice\n",
    "length = [i.shape[-1] for i in test_vital]\n",
    "all_train_id = list(mimic_target['test'].keys())\n",
    "stayids = [all_train_id[i] for i, m in enumerate(length) if m >24]\n",
    "sofa_tail = [mimic_target['test'][j][24:]/15 for j in stayids]\n",
    "# list of array [array([5]), array([1]), array([4]),\n",
    "train_target = [np.nonzero(mimic_static[static_key].loc[idx[:, :, j]].iloc[:, 21:].values)[1] for j in stayids]\n",
    "sub_ind = [i for i, m in enumerate(train_target) if m == 2 or m == 5]\n",
    "race_dict = {2: 1, 5:0}\n",
    "# a list of target class\n",
    "train_targets = [race_dict[train_target[i][0]] for i in sub_ind]\n",
    "# train_filters = [train_filter[i] for i in sub_ind]\n",
    "# sofa_tails = [sofa_tail[i] for i in sub_ind]\n",
    "# stayidss = [stayids[i] for i in sub_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib \n",
    "importlib.reload(utils)\n",
    "train_head, train_static, train_sofa, train_id =  utils.crop_data_target('mimic', train_vital, mimic_target, mimic_static, 'train', args.sens_ind)\n",
    "dev_head, dev_static, dev_sofa, dev_id =  utils.crop_data_target('mimic', dev_vital , mimic_target, mimic_static, 'dev',  args.sens_ind)\n",
    "test_head, test_static, test_sofa, test_id =  utils.crop_data_target('mimic', test_vital, mimic_target, mimic_static, 'test',  args.sens_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.use_sepsis3 == True:\n",
    "    train_head, train_static, train_sofa, train_id = utils.filter_sepsis('mimic', train_head, train_static, train_sofa, train_id, args.platform)\n",
    "    dev_head, dev_static, dev_sofa, dev_id = utils.filter_sepsis('mimic', dev_head, dev_static, dev_sofa, dev_id, args.platform)\n",
    "    test_head, test_static, test_sofa, test_id = utils.filter_sepsis('mimic', test_head, test_static, test_sofa, test_id, args.platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_head = train_head + dev_head\n",
    "trainval_static = train_static + dev_static\n",
    "trainval_stail = train_sofa + dev_sofa\n",
    "trainval_ids = train_id + dev_id\n",
    "\n",
    "# prepare data\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "for c_fold, (train_index, test_index) in enumerate(kf.split(trainval_head)):\n",
    "    # best_loss = 1e4\n",
    "    # patience = 0\n",
    "    # if c_fold >= 1:\n",
    "    #     model.load_state_dict(torch.load('/home/weiliao/FR-TSVAE/start_weights.pt'))\n",
    "    print('Starting Fold %d' % c_fold)\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    train_head, val_head = utils.slice_data(trainval_head, train_index), utils.slice_data(trainval_head, test_index)\n",
    "    train_static, val_static = utils.slice_data(trainval_static, train_index), utils.slice_data(trainval_static, test_index)\n",
    "    train_stail, val_stail = utils.slice_data(trainval_stail, train_index), utils.slice_data(trainval_stail, test_index)\n",
    "    train_id, val_id = utils.slice_data(trainval_ids, train_index), utils.slice_data(trainval_ids, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the return id and the sensitive labels are aligned\n",
    "import random \n",
    "id_20 = random.choices(test_id, k=20)\n",
    "id_20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(prepare_data)\n",
    "train_dataloader, dev_dataloader, test_dataloader = prepare_data.get_data_loader(args, train_head, val_head,\n",
    "                                                                                            test_head, \n",
    "                                                                                            train_stail, val_stail,\n",
    "                                                                                            test_sofa,\n",
    "                                                                                            train_static=train_static,\n",
    "                                                                                            dev_static=val_static,\n",
    "                                                                                            test_static=test_static,\n",
    "                                                                                            train_id=train_id,\n",
    "                                                                                            dev_id=val_id,\n",
    "                                                                                            test_id=test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = []\n",
    "s = []\n",
    "t = []\n",
    "i = []\n",
    "k = []\n",
    "\n",
    "for vitals, static, target, train_ids, key_mask in train_dataloader:\n",
    "    v.append(vitals)\n",
    "    s.append(static)\n",
    "    t.append(target)\n",
    "    i.append(train_ids)\n",
    "    k.append(key_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_s = torch.cat(s, dim=0)\n",
    "total_i = torch.cat(i, dim=0)\n",
    "ind_20 = random.choices(range(len(total_i)), k=20)\n",
    "for j in ind_20: \n",
    "    curr = total_i[j]\n",
    "    print(\"current id is %d \"%curr)\n",
    "    if len(mimic_static['static_train'].loc[idx[:, :, curr.item()]]) == 1: \n",
    "        print(mimic_static['static_train'].loc[idx[:, :, curr.item()]].iloc[:, 0].values[0], mimic_static['static_train'].loc[idx[:, :, curr.item()]].iloc[:, 1].values[0], \n",
    "              mimic_static['static_train'].loc[idx[:, :, curr.item()]].iloc[:, 21:].values[0])\n",
    "    else:\n",
    "        print(mimic_static['static_dev'].loc[idx[:, :, curr.item()]].iloc[:, 0].values[0], mimic_static['static_dev'].loc[idx[:, :, curr.item()]].iloc[:, 1].values[0], \n",
    "              mimic_static['static_dev'].loc[idx[:, :, curr.item()]].iloc[:, 21:].values[0])   \n",
    "    \n",
    "    print(total_s[j])\n",
    "    \n",
    "#     np.nonzero(.values)[1] for j in stayids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_static['static_test'].loc[idx[:, :, curr.item()]].iloc[:, 21:].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mimic_static['static_train'].loc[idx[:, :, curr.item()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_static['static_test'].loc[idx[:, :, 38775862]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimic_static['static_test'].loc[idx[:, :, curr.item()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with CM and AUC plots\n",
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns \n",
    "import torch.nn as nn \n",
    "\n",
    "for p in all_path: \n",
    "    model.load_state_dict(torch.load(p))\n",
    "    print(p)\n",
    "    #  test on test loader \n",
    "    model.eval()\n",
    "    logits = []\n",
    "    stt = []\n",
    "    with torch.no_grad():\n",
    "        for vitals, static, target, train_ids, key_mask in test_dataloader:\n",
    "            # (bs, feature_dim, T)\n",
    "            vitals = vitals.to(device)\n",
    "            # (bs)\n",
    "            static = static.to(device)\n",
    "            # (bs, T, 1)\n",
    "            target = target.to(device)\n",
    "            # (bs, T)\n",
    "            key_mask = key_mask.to(device)\n",
    "\n",
    "            # _mu shape [bs, zdim, T]\n",
    "            _mu, _logvar = model.encoder(vitals)\n",
    "            # b_logits [bs, 1]\n",
    "            b_logits = _mu[:, model.sens_idx]\n",
    "            logits.extend(torch.stack([b_logits[i].squeeze(0).mean() for i in range(len(b_logits))]))\n",
    "            stt.extend(static)\n",
    "    logits = torch.stack(logits)\n",
    "    stt = torch.stack(stt) \n",
    "    metrics.RocCurveDisplay.from_predictions(stt.cpu(),  nn.Sigmoid()(logits).cpu())  \n",
    "    plt.show()\n",
    "    wname = p.split('/')[-1].split('.')[0]\n",
    "    # plt.savefig('./checkpoints/' + workname + 'auc_curve/' + '%s_auc.eps'%wname, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "    pred =  (nn.Sigmoid()(logits).cpu() > 0.5).float()\n",
    "    cm = metrics.confusion_matrix(stt.cpu(), pred)\n",
    "    cf_matrix = cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
    "    # percentage based on true label \n",
    "    gr = (cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)).flatten()\n",
    "    group_percentages = ['{0:.2%}'.format(value) for value in gr]\n",
    "\n",
    "    labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, group_counts)]\n",
    "\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "\n",
    "    xlabel = ['Pred-%d'%i for i in range(2)]\n",
    "    ylabel = ['%d'%i for i in range(2)]\n",
    "\n",
    "    sns.set(font_scale = 1.5)\n",
    "\n",
    "    hm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap = 'OrRd', \\\n",
    "    annot_kws={\"fontsize\": 16}, xticklabels=xlabel, yticklabels=ylabel, cbar=False)\n",
    "    # hm.set(title=title)\n",
    "    fig = plt.gcf()\n",
    "    plt.show()  \n",
    "    # plt.savefig('./checkpoints/' + workname + 'cm_matrix/' + '%s_cm.eps'%wname, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOFA Test set results, including MSE, analyze on 20 paticular patients, MSE variation by ICU stay length \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the same with second stage SOFA training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the 3 sense case\n",
    "import argparse\n",
    "import os \n",
    "import json\n",
    "import glob \n",
    "import copy \n",
    "import pickle\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "mse_loss = nn.MSELoss()\n",
    "import utils\n",
    "from utils import AverageMeterSet\n",
    "import prepare_data\n",
    "import models\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "date = today.strftime(\"%m%d\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import seaborn as sns \n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "legend_properties = {'weight':'bold', 'size': 14}\n",
    "legend_properties_s = {'weight':'bold', 'size': 10}\n",
    "dir_data = {'satori': '/nobackup/users/weiliao', 'colab':'/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_sep_2022/0910'}\n",
    "# load data\n",
    "meep_mimic = np.load(dir_data['satori'] + '/MIMIC_compile_0911_2022.npy', \\\n",
    "                allow_pickle=True).item()\n",
    "train_vital = meep_mimic ['train_head']\n",
    "dev_vital = meep_mimic ['dev_head']\n",
    "test_vital = meep_mimic ['test_head']\n",
    "mimic_static = np.load(dir_data['satori'] + '/MIMIC_static_0922_2022.npy', \\\n",
    "                        allow_pickle=True).item()\n",
    "mimic_target = np.load(dir_data['satori'] + '/MIMIC_target_0922_2022.npy', \\\n",
    "                        allow_pickle=True).item()\n",
    " \n",
    "class Args:\n",
    "    def __init__(self, d=None):\n",
    "        if d is not None:\n",
    "            for key, value in d.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "base_dir = '/home/weiliao/FR-TSVAE/checkpoints/'\n",
    "workname_list = [\n",
    "    '0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens'\n",
    "]\n",
    "\n",
    "with_mask = True\n",
    "for wn in workname_list:\n",
    "\n",
    "    all_path = glob.glob(base_dir + wn + '/*.pt')\n",
    "    # all_path = [p for p in all_path if \"stage2\" not in p]\n",
    "    with open(base_dir+wn+'/params.json') as f: \n",
    "        params = json.load(f)\n",
    "    params['platform'] = 'satori'\n",
    "    params['device_id'] = 0 \n",
    "    args = Args(params)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_head, train_static, train_sofa, train_id =  utils.crop_data_target('mimic', train_vital, mimic_target, mimic_static, 'train', args.sens_ind)\n",
    "    dev_head, dev_static, dev_sofa, dev_id =  utils.crop_data_target('mimic', dev_vital , mimic_target, mimic_static, 'dev',  args.sens_ind)\n",
    "    test_head, test_static, test_sofa, test_id =  utils.crop_data_target('mimic', test_vital, mimic_target, mimic_static, 'test',  args.sens_ind)\n",
    "\n",
    "    if args.use_sepsis3 == True:\n",
    "        train_head, train_static, train_sofa, train_id = utils.filter_sepsis('mimic', train_head, train_static, train_sofa, train_id, args.platform)\n",
    "        dev_head, dev_static, dev_sofa, dev_id = utils.filter_sepsis('mimic', dev_head, dev_static, dev_sofa, dev_id, args.platform)\n",
    "        test_head, test_static, test_sofa, test_id = utils.filter_sepsis('mimic', test_head, test_static, test_sofa, test_id, args.platform)\n",
    "\n",
    "    # build model\n",
    "    model = models.Ffvae(args)\n",
    "    # torch.save(model.state_dict(), '/home/weiliao/FR-TSVAE/start_weights.pt')\n",
    "\n",
    "    # 10-fold cross validation\n",
    "    trainval_head = train_head + dev_head\n",
    "    trainval_static = train_static + dev_static\n",
    "    trainval_stail = train_sofa + dev_sofa\n",
    "    trainval_ids = train_id + dev_id\n",
    "\n",
    "    # prepare data\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for c_fold, (train_index, test_index) in enumerate(kf.split(trainval_head)):\n",
    "        # best_loss = 1e4\n",
    "        # patience = 0\n",
    "        # if c_fold >= 1:\n",
    "        #     model.load_state_dict(torch.load('/home/weiliao/FR-TSVAE/start_weights.pt'))\n",
    "        print('Starting Fold %d' % c_fold)\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        train_head, val_head = utils.slice_data(trainval_head, train_index), utils.slice_data(trainval_head, test_index)\n",
    "        train_static, val_static = utils.slice_data(trainval_static, train_index), utils.slice_data(trainval_static, test_index)\n",
    "        train_stail, val_stail = utils.slice_data(trainval_stail, train_index), utils.slice_data(trainval_stail, test_index)\n",
    "        train_id, val_id = utils.slice_data(trainval_ids, train_index), utils.slice_data(trainval_ids, test_index)\n",
    "\n",
    "        train_dataloader, dev_dataloader, test_dataloader = prepare_data.get_data_loader(args, train_head, val_head,\n",
    "                                                                                            test_head, \n",
    "                                                                                            train_stail, val_stail,\n",
    "                                                                                            test_sofa,\n",
    "                                                                                            train_static=train_static,\n",
    "                                                                                            dev_static=val_static,\n",
    "                                                                                            test_static=test_static,\n",
    "                                                                                            train_id=train_id,\n",
    "                                                                                            dev_id=val_id,\n",
    "                                                                                            test_id=test_id)\n",
    "    # prepare id_20 switch 34590507 to 32882608\n",
    "    stay_ids = [33013986, 37338822, 31972580, 35364108, 34994922, \n",
    "                32087563, 36691371, 31438123, 33266445, 36424894, \n",
    "                32613134, 38247671, 33280018, 33676100, 30932864, \n",
    "                30525046, 33267162, 36431990, 31303162, 37216041]\n",
    "    idmap = {}\n",
    "    for i, ids in enumerate(test_id): \n",
    "        idmap[ids] = i \n",
    "    # convert id to index \n",
    "    id_20 = []\n",
    "    for ids in stay_ids: \n",
    "        id_20.append(idmap[ids])\n",
    "    # deal with CM and AUC plots, 3d \n",
    "    for p in all_path: \n",
    "        # creat a subfolder to save the test results\n",
    "        curr_dir = base_dir + wn + '/'+ p.split('/')[-1].split('.')[0]\n",
    "        if not os.path.exists(curr_dir):\n",
    "            os.makedirs(curr_dir)\n",
    "        model.load_state_dict(torch.load(p, map_location='cuda:0'))\n",
    "        print(p)\n",
    "        #  test on test loader \n",
    "        model.eval()\n",
    "        logits = []\n",
    "        stt = []\n",
    "        sofa_loss = []\n",
    "        with torch.no_grad():\n",
    "            for vitals, static, target, train_ids, key_mask in test_dataloader:\n",
    "                # (bs, feature_dim, T)\n",
    "                vitals = vitals.to(device)\n",
    "                # (bs, 3)\n",
    "                static = static.to(device)\n",
    "                # (bs, T, 1)\n",
    "                target = target.to(device)\n",
    "                # (bs, T)\n",
    "                key_mask = key_mask.to(device)\n",
    "\n",
    "                # _mu shape [bs, zdim, T]\n",
    "                _mu, _logvar = model.encoder(vitals)\n",
    "                # b_logits [bs, 3, T]\n",
    "                b_logits = _mu[:, model.sens_idx, :]\n",
    "                mu = _mu[:, model.nonsens_idx, :]\n",
    "                # (bs, T, 1)\n",
    "                sofa_p = model.regr(mu.transpose(1, 2), \"classify\")\n",
    "                sofa_loss.extend([mse_loss(sofa_p[i][key_mask[i]==0], target[i][key_mask[i]==0]) for i in range(len(sofa_p))])\n",
    "                # for static info prediction \n",
    "                if with_mask: \n",
    "                    logits.extend(torch.stack([b_logits[i][:, key_mask[i]==0].mean(dim=-1)  for i in range(len(b_logits))]))\n",
    "                else:\n",
    "                    logits.extend(torch.stack([b_logits[i].squeeze(0).mean() for i in range(len(b_logits))]))\n",
    "                stt.extend(static)\n",
    "        # save the sofa test result to file \n",
    "        test_loss = torch.mean(torch.stack(sofa_loss)).cpu().numpy()\n",
    "        # compute SOFA loss CI \n",
    "        n_bootstraps = 1000\n",
    "        size_bstr = 3000\n",
    "        loss_np = torch.stack(sofa_loss).cpu().numpy()\n",
    "        bootstrapped_losses = []\n",
    "        for i in range(n_bootstraps):\n",
    "            b = random.choices(loss_np, k=size_bstr)\n",
    "            bootstrapped_losses.append(np.mean(b))\n",
    "        bootstrapped_losses.sort()\n",
    "        # a 95% confidence interval\n",
    "        loss_ci_l = bootstrapped_losses[int(0.025 * len(bootstrapped_losses))]\n",
    "        loss_ci_h = bootstrapped_losses[int(0.975 * len(bootstrapped_losses))]\n",
    "        \n",
    "        logits = torch.stack(logits)\n",
    "        stt = torch.stack(stt) \n",
    "        # AUC CM section needs enumeration \n",
    "        for sens_i, sens_ind in enumerate([0, 1, 21]):\n",
    "            # display AUC \n",
    "            metrics.RocCurveDisplay.from_predictions(stt[:, sens_i].cpu(),  nn.Sigmoid()(logits[:, sens_i]).cpu())  \n",
    "#             fig = plt.gcf()\n",
    "#             plt.show()\n",
    "#             fig.savefig(curr_dir + '/auc.eps', format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "            # calculate optimal threshold\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(stt[:, sens_i].cpu(),  nn.Sigmoid()(logits[:, sens_i]).cpu())\n",
    "            gmeans = np.sqrt(tpr * (1-fpr))\n",
    "            opt_th = thresholds[np.argmax(gmeans)]\n",
    "\n",
    "            # AUC CI \n",
    "            bootstrapped_aucs = []\n",
    "            for i in range(n_bootstraps):\n",
    "                # bootstrap by sampling with replacement on the prediction indices\n",
    "                indices = random.choices(range(len(stt)), k=1000)\n",
    "                score = metrics.roc_auc_score(stt[:, sens_i].cpu()[indices], nn.Sigmoid()(logits[:, sens_i]).cpu()[indices])\n",
    "                bootstrapped_aucs.append(score)\n",
    "            bootstrapped_aucs.sort()\n",
    "            # a 95% confidence interval\n",
    "            auc_ci_l = bootstrapped_aucs[int(0.025 * len(bootstrapped_aucs))]\n",
    "            auc_ci_h = bootstrapped_aucs[int(0.975 * len(bootstrapped_aucs))]\n",
    "            msg = 'test loss %.5f, sofa loss ci (%.5f - %.5f), auc ci (%.5f - %.5f)'%(test_loss, loss_ci_l, loss_ci_h, auc_ci_l, auc_ci_h)\n",
    "            print(msg)\n",
    "\n",
    "#             with open(curr_dir + '/sofa_test.json', 'w') as f:\n",
    "#                 msg = 'test loss %.5f, sofa loss ci (%.5f - %.5f), auc ci (%.5f - %.5f)'%(test_loss, loss_ci_l, loss_ci_h, auc_ci_l, auc_ci_h)\n",
    "#                 json.dump(msg, f)\n",
    "\n",
    "            for th in [0.5, opt_th]:\n",
    "\n",
    "                pred =  (nn.Sigmoid()(logits[:, sens_i]).cpu() > th).float()\n",
    "                cm = metrics.confusion_matrix(stt[:, sens_i].cpu(), pred)\n",
    "                cf_matrix = cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)\n",
    "                group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
    "                # percentage based on true label \n",
    "                gr = (cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)).flatten()\n",
    "                group_percentages = ['{0:.2%}'.format(value) for value in gr]\n",
    "\n",
    "                labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, group_counts)]\n",
    "\n",
    "                labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "                if sens_ind == 0:\n",
    "                    xlabel = ['Pred-%s'%l for l in ['F', 'M']]\n",
    "                    ylabel = ['%s'%l for l in ['F', 'M']]   \n",
    "                elif sens_ind == 1: \n",
    "                    xlabel = ['Pred-%s'%l for l in ['Y', 'E']]\n",
    "                    ylabel = ['%s'%l for l in ['Y', 'E']]   \n",
    "                elif sens_ind == 21: \n",
    "                    xlabel = ['Pred-%s'%l for l in ['W', 'B']]\n",
    "                    ylabel = ['%s'%l for l in ['W', 'B']]   \n",
    "\n",
    "                sns.set(font_scale = 1.5)\n",
    "\n",
    "                hm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap = 'OrRd', \\\n",
    "                annot_kws={\"fontsize\": 16}, xticklabels=xlabel, yticklabels=ylabel, cbar=False)\n",
    "                # hm.set(title=title)\n",
    "#                 fig = plt.gcf()\n",
    "#                 plt.show()  \n",
    "#                 fig.savefig(curr_dir + '/cm_%f.eps'%th, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "        \n",
    "        for v in ['raw', 'smooth']: \n",
    "            fig, ax = plt.subplots(4, 5, figsize=(25, 12))\n",
    "            axes = ax.flatten()\n",
    "            for i in range(20):\n",
    "                id = test_id[id_20[i]]\n",
    "                sofa = mimic_target['test'][id]\n",
    "                _mu, _logvar = model.encoder(torch.FloatTensor(test_head[id_20[i]]).unsqueeze(0).to(device))\n",
    "                mu = _mu[:, model.nonsens_idx, :]\n",
    "                sofa_p = model.regr(mu.transpose(1, 2), \"classify\").squeeze(0).cpu().detach().numpy()*15\n",
    "                if v == 'smooth': \n",
    "                    sofa_p = [np.round(i) for i in sofa_p]\n",
    "                n = len(sofa)\n",
    "                axes[i].plot(range(len(sofa)), sofa, label='Current SOFA')\n",
    "                axes[i].plot(range(24, n), sofa_p, c=\"tab:green\", label ='Predicted SOFA')\n",
    "                axes[i].set_xlim((0, len(sofa)))\n",
    "                axes[i].tick_params(axis='both', labelsize=8)\n",
    "                if max(sofa) <= 11 and int(max(sofa_p)) <=11:\n",
    "                    axes[i].set_ylim((0, 13))\n",
    "                else: \n",
    "                    axes[i].set_ylim((0, max(max(sofa), int(max(sofa_p)))+2))\n",
    "                if i == 0: \n",
    "                    axes[i].set_ylabel('SOFA score', size=14,  fontweight='bold')\n",
    "                if i == 19:\n",
    "                    axes[i].set_xlabel('ICU_in Hours', size=14,  fontweight='bold')\n",
    "                if i == 4:\n",
    "                    axes[i].legend(loc='upper right',  prop=legend_properties)\n",
    "                # save each small figure \n",
    "                fig_s, ax_s = plt.subplots(1, 1, figsize=(5, 3))\n",
    "                ax_s.plot(range(len(sofa)), sofa, label='Current SOFA')\n",
    "                ax_s.plot(range(24, n), sofa_p, c=\"tab:green\", label ='Predicted SOFA')\n",
    "                ax_s.set_xlim((0, len(sofa)))\n",
    "                ax_s.tick_params(axis='both', labelsize=6)\n",
    "                if max(sofa) <= 11 and int(max(sofa_p)) <=11:\n",
    "                    ax_s.set_ylim((0, 13))\n",
    "                else: \n",
    "                    ax_s.set_ylim((0, max(max(sofa), int(max(sofa_p)))+2))\n",
    "                ax_s.set_ylabel('SOFA score', size=12,  fontweight='bold')\n",
    "                ax_s.set_xlabel('ICU_in Hours', size=12,  fontweight='bold')\n",
    "                ax_s.legend(loc='upper right',  prop=legend_properties_s)\n",
    "#                 fig_s.savefig(curr_dir + '/indiv_sofa_%s_%d.eps'%(v, i), format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "#             # save the big figure \n",
    "#             fig.savefig(curr_dir + '/sofa_%.5f_%s.eps'%(test_loss, v), format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fold 0\n",
      "TRAIN: 8964 TEST: 2242\n",
      "8964\n",
      "Starting Fold 1\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "Starting Fold 2\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "Starting Fold 3\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "Starting Fold 4\n",
      "TRAIN: 8965 TEST: 2241\n",
      "8965\n",
      "/home/weiliao/FR-TSVAE/checkpoints/0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens/stage1_clfw_fold_0_epoch149.pt\n",
      "test loss 0.01601, sofa loss ci (0.01514 - 0.01689), auc ci (0.71922 - 0.78086)\n",
      "test loss 0.01601, sofa loss ci (0.01514 - 0.01689), auc ci (0.73456 - 0.79252)\n",
      "test loss 0.01601, sofa loss ci (0.01514 - 0.01689), auc ci (0.70116 - 0.79493)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nobackup/users/weiliao/anaconda3/envs/project/lib/python3.7/site-packages/ipykernel_launcher.py:277: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/weiliao/FR-TSVAE/checkpoints/0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens/stage1_corr_fold_0_epoch149.pt\n",
      "test loss 0.01410, sofa loss ci (0.01335 - 0.01499), auc ci (0.75009 - 0.80580)\n",
      "test loss 0.01410, sofa loss ci (0.01335 - 0.01499), auc ci (0.75196 - 0.81071)\n",
      "test loss 0.01410, sofa loss ci (0.01335 - 0.01499), auc ci (0.49296 - 0.61692)\n",
      "/home/weiliao/FR-TSVAE/checkpoints/0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens/stage1_fold_0_epoch149.pt\n",
      "test loss 0.01388, sofa loss ci (0.01319 - 0.01467), auc ci (0.71852 - 0.77926)\n",
      "test loss 0.01388, sofa loss ci (0.01319 - 0.01467), auc ci (0.74256 - 0.80050)\n",
      "test loss 0.01388, sofa loss ci (0.01319 - 0.01467), auc ci (0.47569 - 0.60137)\n",
      "/home/weiliao/FR-TSVAE/checkpoints/0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens/stage1_clf_fold_0_epoch149.pt\n",
      "test loss 0.01497, sofa loss ci (0.01425 - 0.01574), auc ci (0.71177 - 0.76787)\n",
      "test loss 0.01497, sofa loss ci (0.01425 - 0.01574), auc ci (0.71425 - 0.77223)\n",
      "test loss 0.01497, sofa loss ci (0.01425 - 0.01574), auc ci (0.63016 - 0.73148)\n",
      "/home/weiliao/FR-TSVAE/checkpoints/0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens/stage1_sofa_fold_0_epoch149.pt\n",
      "test loss 0.01381, sofa loss ci (0.01306 - 0.01456), auc ci (0.73260 - 0.79044)\n",
      "test loss 0.01381, sofa loss ci (0.01306 - 0.01456), auc ci (0.74618 - 0.80337)\n",
      "test loss 0.01381, sofa loss ci (0.01306 - 0.01456), auc ci (0.49240 - 0.62051)\n"
     ]
    }
   ],
   "source": [
    "# 3 sens version 2 with weights \n",
    "# check the 3 sense case\n",
    "import argparse\n",
    "import os \n",
    "import json\n",
    "import glob \n",
    "import copy \n",
    "import pickle\n",
    "import random \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "mse_loss = nn.MSELoss()\n",
    "import utils\n",
    "from utils import AverageMeterSet\n",
    "import prepare_data\n",
    "import models\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "kf = KFold(n_splits=5, random_state=None, shuffle=False)\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "date = today.strftime(\"%m%d\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib \n",
    "import seaborn as sns \n",
    "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams[\"font.weight\"] = \"bold\"\n",
    "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
    "legend_properties = {'weight':'bold', 'size': 14}\n",
    "legend_properties_s = {'weight':'bold', 'size': 10}\n",
    "dir_data = {'satori': '/nobackup/users/weiliao', 'colab':'/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_sep_2022/0910'}\n",
    "# load data\n",
    "meep_mimic = np.load(dir_data['satori'] + '/MIMIC_compile_0911_2022.npy', \\\n",
    "                allow_pickle=True).item()\n",
    "train_vital = meep_mimic ['train_head']\n",
    "dev_vital = meep_mimic ['dev_head']\n",
    "test_vital = meep_mimic ['test_head']\n",
    "mimic_static = np.load(dir_data['satori'] + '/MIMIC_static_0922_2022.npy', \\\n",
    "                        allow_pickle=True).item()\n",
    "mimic_target = np.load(dir_data['satori'] + '/MIMIC_target_0922_2022.npy', \\\n",
    "                        allow_pickle=True).item()\n",
    " \n",
    "class Args:\n",
    "    def __init__(self, d=None):\n",
    "        if d is not None:\n",
    "            for key, value in d.items():\n",
    "                setattr(self, key, value)\n",
    "\n",
    "base_dir = '/home/weiliao/FR-TSVAE/checkpoints/'\n",
    "workname_list = [\n",
    "    '0519_lr1e-4beta.001_res_regrtheta_1_mlp_regr_nonsens_sens0_mask_3sens'\n",
    "]\n",
    "\n",
    "with_mask = True\n",
    "for wn in workname_list:\n",
    "\n",
    "    all_path = glob.glob(base_dir + wn + '/*.pt')\n",
    "    # all_path = [p for p in all_path if \"stage2\" not in p]\n",
    "    with open(base_dir+wn+'/params.json') as f: \n",
    "        params = json.load(f)\n",
    "    params['platform'] = 'satori'\n",
    "    params['device_id'] = 0 \n",
    "    args = Args(params)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    train_head, train_static, train_sofa, train_id =  utils.crop_data_target('mimic', train_vital, mimic_target, mimic_static, 'train', args.sens_ind)\n",
    "    dev_head, dev_static, dev_sofa, dev_id =  utils.crop_data_target('mimic', dev_vital , mimic_target, mimic_static, 'dev',  args.sens_ind)\n",
    "    test_head, test_static, test_sofa, test_id =  utils.crop_data_target('mimic', test_vital, mimic_target, mimic_static, 'test',  args.sens_ind)\n",
    "\n",
    "    if args.use_sepsis3 == True:\n",
    "        train_head, train_static, train_sofa, train_id = utils.filter_sepsis('mimic', train_head, train_static, train_sofa, train_id, args.platform)\n",
    "        dev_head, dev_static, dev_sofa, dev_id = utils.filter_sepsis('mimic', dev_head, dev_static, dev_sofa, dev_id, args.platform)\n",
    "        test_head, test_static, test_sofa, test_id = utils.filter_sepsis('mimic', test_head, test_static, test_sofa, test_id, args.platform)\n",
    "\n",
    "    # build model\n",
    "   \n",
    "    # torch.save(model.state_dict(), '/home/weiliao/FR-TSVAE/start_weights.pt')\n",
    "\n",
    "    # 10-fold cross validation\n",
    "    trainval_head = train_head + dev_head\n",
    "    trainval_static = train_static + dev_static\n",
    "    trainval_stail = train_sofa + dev_sofa\n",
    "    trainval_ids = train_id + dev_id\n",
    "\n",
    "    # prepare data\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    for c_fold, (train_index, test_index) in enumerate(kf.split(trainval_head)):\n",
    "        # best_loss = 1e4\n",
    "        # patience = 0\n",
    "        # if c_fold >= 1:\n",
    "        #     model.load_state_dict(torch.load('/home/weiliao/FR-TSVAE/start_weights.pt'))\n",
    "        print('Starting Fold %d' % c_fold)\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        train_head, val_head = utils.slice_data(trainval_head, train_index), utils.slice_data(trainval_head, test_index)\n",
    "        train_static, val_static = utils.slice_data(trainval_static, train_index), utils.slice_data(trainval_static, test_index)\n",
    "        train_stail, val_stail = utils.slice_data(trainval_stail, train_index), utils.slice_data(trainval_stail, test_index)\n",
    "        train_id, val_id = utils.slice_data(trainval_ids, train_index), utils.slice_data(trainval_ids, test_index)\n",
    "        weights_per_class = []\n",
    "        for i in range(3):\n",
    "            ctype, count= np.unique(np.asarray(val_static)[:, i], return_counts=True)\n",
    "            total_dev_samples = len(val_static)\n",
    "            curr = torch.FloatTensor([ total_dev_samples / k / len(ctype) for k in count]).to(device)\n",
    "            weights_per_class.append(curr)\n",
    "\n",
    "        train_dataloader, dev_dataloader, test_dataloader = prepare_data.get_data_loader(args, train_head, val_head,\n",
    "                                                                                            test_head, \n",
    "                                                                                            train_stail, val_stail,\n",
    "                                                                                            test_sofa,\n",
    "                                                                                            train_static=train_static,\n",
    "                                                                                            dev_static=val_static,\n",
    "                                                                                            test_static=test_static,\n",
    "                                                                                            train_id=train_id,\n",
    "                                                                                            dev_id=val_id,\n",
    "                                                                                            test_id=test_id)\n",
    "        model = models.Ffvae(args, weights_per_class)\n",
    "    # prepare id_20 switch 34590507 to 32882608\n",
    "    stay_ids = [33013986, 37338822, 31972580, 35364108, 34994922, \n",
    "                32087563, 36691371, 31438123, 33266445, 36424894, \n",
    "                32613134, 38247671, 33280018, 33676100, 30932864, \n",
    "                30525046, 33267162, 36431990, 31303162, 37216041]\n",
    "    idmap = {}\n",
    "    for i, ids in enumerate(test_id): \n",
    "        idmap[ids] = i \n",
    "    # convert id to index \n",
    "    id_20 = []\n",
    "    for ids in stay_ids: \n",
    "        id_20.append(idmap[ids])\n",
    "    # deal with CM and AUC plots, 3d \n",
    "    for p in all_path: \n",
    "        # creat a subfolder to save the test results\n",
    "        curr_dir = base_dir + wn + '/'+ p.split('/')[-1].split('.')[0]\n",
    "        if not os.path.exists(curr_dir):\n",
    "            os.makedirs(curr_dir)\n",
    "        model.load_state_dict(torch.load(p, map_location='cuda:0'))\n",
    "        print(p)\n",
    "        #  test on test loader \n",
    "        model.eval()\n",
    "        logits = []\n",
    "        stt = []\n",
    "        sofa_loss = []\n",
    "        with torch.no_grad():\n",
    "            for vitals, static, target, train_ids, key_mask in test_dataloader:\n",
    "                # (bs, feature_dim, T)\n",
    "                vitals = vitals.to(device)\n",
    "                # (bs, 3)\n",
    "                static = static.to(device)\n",
    "                # (bs, T, 1)\n",
    "                target = target.to(device)\n",
    "                # (bs, T)\n",
    "                key_mask = key_mask.to(device)\n",
    "\n",
    "                # _mu shape [bs, zdim, T]\n",
    "                _mu, _logvar = model.encoder(vitals)\n",
    "                # b_logits [bs, 3, T]\n",
    "                b_logits = _mu[:, model.sens_idx, :]\n",
    "                mu = _mu[:, model.nonsens_idx, :]\n",
    "                # (bs, T, 1)\n",
    "                sofa_p = model.regr(mu.transpose(1, 2), \"classify\")\n",
    "                sofa_loss.extend([mse_loss(sofa_p[i][key_mask[i]==0], target[i][key_mask[i]==0]) for i in range(len(sofa_p))])\n",
    "                # for static info prediction \n",
    "                if with_mask: \n",
    "                    logits.extend(torch.stack([b_logits[i][:, key_mask[i]==0].mean(dim=-1)  for i in range(len(b_logits))]))\n",
    "                else:\n",
    "                    logits.extend(torch.stack([b_logits[i].squeeze(0).mean() for i in range(len(b_logits))]))\n",
    "                stt.extend(static)\n",
    "        # save the sofa test result to file \n",
    "        test_loss = torch.mean(torch.stack(sofa_loss)).cpu().numpy()\n",
    "        # compute SOFA loss CI \n",
    "        n_bootstraps = 1000\n",
    "        size_bstr = 3000\n",
    "        loss_np = torch.stack(sofa_loss).cpu().numpy()\n",
    "        bootstrapped_losses = []\n",
    "        for i in range(n_bootstraps):\n",
    "            b = random.choices(loss_np, k=size_bstr)\n",
    "            bootstrapped_losses.append(np.mean(b))\n",
    "        bootstrapped_losses.sort()\n",
    "        # a 95% confidence interval\n",
    "        loss_ci_l = bootstrapped_losses[int(0.025 * len(bootstrapped_losses))]\n",
    "        loss_ci_h = bootstrapped_losses[int(0.975 * len(bootstrapped_losses))]\n",
    "        \n",
    "        logits = torch.stack(logits)\n",
    "        stt = torch.stack(stt) \n",
    "        # AUC CM section needs enumeration \n",
    "        for sens_i, sens_ind in enumerate([0, 1, 21]):\n",
    "            # display AUC \n",
    "            metrics.RocCurveDisplay.from_predictions(stt[:, sens_i].cpu(),  nn.Sigmoid()(logits[:, sens_i]).cpu())  \n",
    "#             fig = plt.gcf()\n",
    "#             plt.show()\n",
    "#             fig.savefig(curr_dir + '/auc.eps', format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "            # calculate optimal threshold\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(stt[:, sens_i].cpu(),  nn.Sigmoid()(logits[:, sens_i]).cpu())\n",
    "            gmeans = np.sqrt(tpr * (1-fpr))\n",
    "            opt_th = thresholds[np.argmax(gmeans)]\n",
    "            all_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "            # AUC CI \n",
    "            bootstrapped_aucs = []\n",
    "            for i in range(n_bootstraps):\n",
    "                # bootstrap by sampling with replacement on the prediction indices\n",
    "                indices = random.choices(range(len(stt)), k=1000)\n",
    "                score = metrics.roc_auc_score(stt[:, sens_i].cpu()[indices], nn.Sigmoid()(logits[:, sens_i]).cpu()[indices])\n",
    "                bootstrapped_aucs.append(score)\n",
    "            bootstrapped_aucs.sort()\n",
    "            # a 95% confidence interval\n",
    "            auc_ci_l = bootstrapped_aucs[int(0.025 * len(bootstrapped_aucs))]\n",
    "            auc_ci_h = bootstrapped_aucs[int(0.975 * len(bootstrapped_aucs))]\n",
    "            msg = 'test loss %.5f, auc %.5f, sofa loss ci (%.5f - %.5f), auc ci (%.5f - %.5f)'%(test_loss, all_auc, loss_ci_l, loss_ci_h, auc_ci_l, auc_ci_h)\n",
    "            print(msg)\n",
    "\n",
    "#             with open(curr_dir + '/sofa_test.json', 'w') as f:\n",
    "#                 msg = 'test loss %.5f, sofa loss ci (%.5f - %.5f), auc ci (%.5f - %.5f)'%(test_loss, loss_ci_l, loss_ci_h, auc_ci_l, auc_ci_h)\n",
    "#                 json.dump(msg, f)\n",
    "\n",
    "            for th in [0.5, opt_th]:\n",
    "\n",
    "                pred =  (nn.Sigmoid()(logits[:, sens_i]).cpu() > th).float()\n",
    "                cm = metrics.confusion_matrix(stt[:, sens_i].cpu(), pred)\n",
    "                cf_matrix = cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)\n",
    "                group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
    "                # percentage based on true label \n",
    "                gr = (cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), 2, axis=1)).flatten()\n",
    "                group_percentages = ['{0:.2%}'.format(value) for value in gr]\n",
    "\n",
    "                labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, group_counts)]\n",
    "\n",
    "                labels = np.asarray(labels).reshape(2, 2)\n",
    "\n",
    "                if sens_ind == 0:\n",
    "                    xlabel = ['Pred-%s'%l for l in ['F', 'M']]\n",
    "                    ylabel = ['%s'%l for l in ['F', 'M']]   \n",
    "                elif sens_ind == 1: \n",
    "                    xlabel = ['Pred-%s'%l for l in ['Y', 'E']]\n",
    "                    ylabel = ['%s'%l for l in ['Y', 'E']]   \n",
    "                elif sens_ind == 21: \n",
    "                    xlabel = ['Pred-%s'%l for l in ['W', 'B']]\n",
    "                    ylabel = ['%s'%l for l in ['W', 'B']]   \n",
    "\n",
    "                sns.set(font_scale = 1.5)\n",
    "\n",
    "                hm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap = 'OrRd', \\\n",
    "                annot_kws={\"fontsize\": 16}, xticklabels=xlabel, yticklabels=ylabel, cbar=False)\n",
    "                # hm.set(title=title)\n",
    "#                 fig = plt.gcf()\n",
    "#                 plt.show()  \n",
    "#                 fig.savefig(curr_dir + '/cm_%f.eps'%th, format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "        \n",
    "        for v in ['raw', 'smooth']: \n",
    "            fig, ax = plt.subplots(4, 5, figsize=(25, 12))\n",
    "            axes = ax.flatten()\n",
    "            for i in range(20):\n",
    "                id = test_id[id_20[i]]\n",
    "                sofa = mimic_target['test'][id]\n",
    "                _mu, _logvar = model.encoder(torch.FloatTensor(test_head[id_20[i]]).unsqueeze(0).to(device))\n",
    "                mu = _mu[:, model.nonsens_idx, :]\n",
    "                sofa_p = model.regr(mu.transpose(1, 2), \"classify\").squeeze(0).cpu().detach().numpy()*15\n",
    "                if v == 'smooth': \n",
    "                    sofa_p = [np.round(i) for i in sofa_p]\n",
    "                n = len(sofa)\n",
    "                axes[i].plot(range(len(sofa)), sofa, label='Current SOFA')\n",
    "                axes[i].plot(range(24, n), sofa_p, c=\"tab:green\", label ='Predicted SOFA')\n",
    "                axes[i].set_xlim((0, len(sofa)))\n",
    "                axes[i].tick_params(axis='both', labelsize=8)\n",
    "                if max(sofa) <= 11 and int(max(sofa_p)) <=11:\n",
    "                    axes[i].set_ylim((0, 13))\n",
    "                else: \n",
    "                    axes[i].set_ylim((0, max(max(sofa), int(max(sofa_p)))+2))\n",
    "                if i == 0: \n",
    "                    axes[i].set_ylabel('SOFA score', size=14,  fontweight='bold')\n",
    "                if i == 19:\n",
    "                    axes[i].set_xlabel('ICU_in Hours', size=14,  fontweight='bold')\n",
    "                if i == 4:\n",
    "                    axes[i].legend(loc='upper right',  prop=legend_properties)\n",
    "                # save each small figure \n",
    "                fig_s, ax_s = plt.subplots(1, 1, figsize=(5, 3))\n",
    "                ax_s.plot(range(len(sofa)), sofa, label='Current SOFA')\n",
    "                ax_s.plot(range(24, n), sofa_p, c=\"tab:green\", label ='Predicted SOFA')\n",
    "                ax_s.set_xlim((0, len(sofa)))\n",
    "                ax_s.tick_params(axis='both', labelsize=6)\n",
    "                if max(sofa) <= 11 and int(max(sofa_p)) <=11:\n",
    "                    ax_s.set_ylim((0, 13))\n",
    "                else: \n",
    "                    ax_s.set_ylim((0, max(max(sofa), int(max(sofa_p)))+2))\n",
    "                ax_s.set_ylabel('SOFA score', size=12,  fontweight='bold')\n",
    "                ax_s.set_xlabel('ICU_in Hours', size=12,  fontweight='bold')\n",
    "                ax_s.legend(loc='upper right',  prop=legend_properties_s)\n",
    "#                 fig_s.savefig(curr_dir + '/indiv_sofa_%s_%d.eps'%(v, i), format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
    "\n",
    "#             # save the big figure \n",
    "#             fig.savefig(curr_dir + '/sofa_%.5f_%s.eps'%(test_loss, v), format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_fold, (train_index, test_index) in enumerate(kf.split(trainval_head)):\n",
    "    # best_loss = 1e4\n",
    "    # patience = 0\n",
    "    # if c_fold >= 1:\n",
    "    #     model.load_state_dict(torch.load('/home/weiliao/FR-TSVAE/start_weights.pt'))\n",
    "    print('Starting Fold %d' % c_fold)\n",
    "    print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "    train_head, val_head = utils.slice_data(trainval_head, train_index), utils.slice_data(trainval_head, test_index)\n",
    "    train_static, val_static = utils.slice_data(trainval_static, train_index), utils.slice_data(trainval_static, test_index)\n",
    "    train_stail, val_stail = utils.slice_data(trainval_stail, train_index), utils.slice_data(trainval_stail, test_index)\n",
    "    train_id, val_id = utils.slice_data(trainval_ids, train_index), utils.slice_data(trainval_ids, test_index)\n",
    "\n",
    "    print(plt.hist(np.asarray(train_static)))\n",
    "    print(plt.hist(np.asarray(val_static)))\n",
    "#     train_dataloader, dev_dataloader, test_dataloader = prepare_data.get_data_loader(args, train_head, val_head,\n",
    "#                                                                                         test_head, \n",
    "#                                                                                         train_stail, val_stail,\n",
    "#                                                                                         test_sofa,\n",
    "#                                                                                         train_static=train_static,\n",
    "#                                                                                         dev_static=val_static,\n",
    "#                                                                                         test_static=test_static,\n",
    "#                                                                                         train_id=train_id,\n",
    "#                                                                                         dev_id=val_id,\n",
    "#                                                                                         test_id=test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_per_class = []\n",
    "for i in range(3):\n",
    "    ctype, count= np.unique(np.asarray(val_static)[:, i], return_counts=True)\n",
    "    total_dev_samples = len(val_static)\n",
    "    curr = torch.FloatTensor([ total_dev_samples / k / len(ctype) for k in count]).to(device)\n",
    "    weights_per_class.append(curr)\n",
    "#     ce_val_loss = nn.CrossEntropyLoss(weight = weights_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.01381, auc 0.55354, sofa loss ci (0.01306 - 0.01456), auc ci (0.49240 - 0.62051)\n"
     ]
    }
   ],
   "source": [
    "all_auc = metrics.auc(fpr, tpr)\n",
    "msg = 'test loss %.5f, auc %.5f, sofa loss ci (%.5f - %.5f), auc ci (%.5f - %.5f)'%(test_loss, all_auc, loss_ci_l, loss_ci_h, auc_ci_l, auc_ci_h)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use b_logits and static \n",
    "b_squeeze.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_squeeze = torch.stack([b_logits[i][:, key_mask[i]==0].mean(dim=-1)  for i in range(len(b_logits))])\n",
    "clf_losses = [\n",
    "            nn.BCEWithLogitsLoss(pos_weight = weights_per_class[k][1]/weights_per_class[k][0])(_b_logit.to(device), _a_sens.to(device))\n",
    "            for k, (_b_logit, _a_sens) in enumerate(zip(\n",
    "            b_squeeze.t(), static.type(torch.FloatTensor).t()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.6690, device='cuda:0'),\n",
       " tensor(0.3711, device='cuda:0'),\n",
       " tensor(0.6934, device='cuda:0')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3125, 0.3751, 0.1742, 0.2965], device='cuda:0') tensor([1., 0., 1., 0.])\n",
      "tensor(0.6690, device='cuda:0')\n",
      "tensor([0.0493, 0.0386, 2.7230, 4.4408], device='cuda:0') tensor([0., 0., 1., 1.])\n",
      "tensor(0.3711, device='cuda:0')\n",
      "tensor([6.1603e-06, 1.7574e-03, 0.0000e+00, 0.0000e+00], device='cuda:0') tensor([0., 0., 0., 0.])\n",
      "tensor(0.6934, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    " for k, (_b_logit, _a_sens) in enumerate(zip(b_squeeze.t(), static.type(torch.FloatTensor).t())):\n",
    "        print(_b_logit, _a_sens)\n",
    "        c = nn.BCEWithLogitsLoss(pos_weight = weights_per_class[k][1]/weights_per_class[k][0])\n",
    "        print(c(_b_logit.to(device), _a_sens.to(device)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_b_logit.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_a_sens.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = torch.ones([64]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7986, device='cuda:0')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
